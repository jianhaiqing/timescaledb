-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
-- Need to be super user to create extension and add data nodes
\c :TEST_DBNAME :ROLE_CLUSTER_SUPERUSER;
\ir include/remote_exec.sql
-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
CREATE SCHEMA IF NOT EXISTS test;
psql:include/remote_exec.sql:5: NOTICE:  schema "test" already exists, skipping
GRANT USAGE ON SCHEMA test TO PUBLIC;
CREATE OR REPLACE FUNCTION test.remote_exec(srv_name name[], command text)
RETURNS VOID
AS :TSL_MODULE_PATHNAME, 'ts_remote_exec'
LANGUAGE C;
CREATE OR REPLACE FUNCTION test_override_pushdown_timestamptz(new_value TIMESTAMPTZ) RETURNS VOID
AS :TSL_MODULE_PATHNAME, 'test_override_pushdown_timestamptz'
LANGUAGE C VOLATILE STRICT;
-- Cleanup from other potential tests that created these databases
SET client_min_messages TO ERROR;
DROP DATABASE IF EXISTS data_node_1;
DROP DATABASE IF EXISTS data_node_2;
SET client_min_messages TO NOTICE;
GRANT USAGE ON FOREIGN DATA WRAPPER timescaledb_fdw TO :ROLE_3;
SET ROLE :ROLE_3;
SELECT inet_server_port() AS "port" \gset
-- Add data nodes using the TimescaleDB node management API
SET ROLE :ROLE_CLUSTER_SUPERUSER;
SELECT * FROM add_data_node('data_node_1', host => 'localhost',
                            database => 'data_node_1');
  node_name  |   host    | port  |  database   | node_created | database_created | extension_created 
-------------+-----------+-------+-------------+--------------+------------------+-------------------
 data_node_1 | localhost | 15432 | data_node_1 | t            | t                | t
(1 row)

SELECT * FROM add_data_node('data_node_2', host => 'localhost',
                            database => 'data_node_2');
  node_name  |   host    | port  |  database   | node_created | database_created | extension_created 
-------------+-----------+-------+-------------+--------------+------------------+-------------------
 data_node_2 | localhost | 15432 | data_node_2 | t            | t                | t
(1 row)

GRANT USAGE ON FOREIGN SERVER data_node_1, data_node_2 TO PUBLIC;
SET ROLE :ROLE_3;
-- Create a 2-dimensional partitioned table for comparision
CREATE TABLE pg2dim (time timestamptz, device int, location int, temp float) PARTITION BY HASH (device);
CREATE TABLE pg2dim_h1 PARTITION OF pg2dim FOR VALUES WITH (MODULUS 2, REMAINDER 0) PARTITION BY RANGE(time);
CREATE TABLE pg2dim_h2 PARTITION OF pg2dim FOR VALUES WITH (MODULUS 2, REMAINDER 1) PARTITION BY RANGE(time);
CREATE TABLE pg2dim_h1_t1 PARTITION OF pg2dim_h1 FOR VALUES FROM ('2018-01-18 00:00') TO ('2018-04-18 00:00');
CREATE TABLE pg2dim_h1_t2 PARTITION OF pg2dim_h1 FOR VALUES FROM ('2018-04-18 00:00') TO ('2018-07-18 00:00');
CREATE TABLE pg2dim_h1_t3 PARTITION OF pg2dim_h1 FOR VALUES FROM ('2018-07-18 00:00') TO ('2018-10-18 00:00');
CREATE TABLE pg2dim_h2_t1 PARTITION OF pg2dim_h2 FOR VALUES FROM ('2018-01-18 00:00') TO ('2018-04-18 00:00');
CREATE TABLE pg2dim_h2_t2 PARTITION OF pg2dim_h2 FOR VALUES FROM ('2018-04-18 00:00') TO ('2018-07-18 00:00');
CREATE TABLE pg2dim_h2_t3 PARTITION OF pg2dim_h2 FOR VALUES FROM ('2018-07-18 00:00') TO ('2018-10-18 00:00');
CREATE TABLE hyper (time timestamptz, device int, location int, temp float);
SELECT * FROM create_distributed_hypertable('hyper', 'time', 'device', 2, chunk_time_interval => '3 months'::interval);
NOTICE:  adding not-null constraint to column "time"
 hypertable_id | schema_name | table_name | created 
---------------+-------------+------------+---------
             1 | public      | hyper      | t
(1 row)

INSERT INTO pg2dim VALUES
       ('2018-01-19 13:01', 1, 2, 2.3),
       ('2018-01-20 15:05', 1, 3, 5.3),
       ('2018-02-21 13:01', 3, 4, 1.5),
       ('2018-02-28 15:05', 1, 1, 5.6),
       ('2018-02-19 13:02', 3, 5, 3.1),
       ('2018-02-19 13:02', 2, 3, 6.7),
       ('2018-03-08 11:05', 6, 2, 8.1),
       ('2018-03-08 11:05', 7, 4, 4.6),
       ('2018-03-10 17:02', 5, 5, 5.1),
       ('2018-03-10 17:02', 1, 6, 9.1),
       ('2018-03-17 12:02', 2, 2, 6.7),
       ('2018-04-19 13:01', 1, 2, 7.6),
       ('2018-04-20 15:08', 5, 5, 6.4),
       ('2018-05-19 13:01', 4, 4, 5.1),
       ('2018-05-20 15:08', 5, 1, 9.4),
       ('2018-05-30 13:02', 3, 2, 9.0),
       ('2018-09-19 13:01', 1, 3, 6.1),
       ('2018-09-20 15:08', 4, 5, 10.4),
       ('2018-09-30 13:02', 3, 4, 9.9);
INSERT INTO hyper VALUES
       ('2018-01-19 13:01', 1, 2, 2.3),
       ('2018-01-20 15:05', 1, 3, 5.3),
       ('2018-02-21 13:01', 3, 4, 1.5),
       ('2018-02-28 15:05', 1, 1, 5.6),
       ('2018-02-19 13:02', 3, 5, 3.1),
       ('2018-02-19 13:02', 2, 3, 6.7),
       ('2018-03-08 11:05', 6, 2, 8.1),
       ('2018-03-08 11:05', 7, 4, 4.6),
       ('2018-03-10 17:02', 5, 5, 5.1),
       ('2018-03-10 17:02', 1, 6, 9.1),
       ('2018-03-17 12:02', 2, 2, 6.7),
       ('2018-04-19 13:01', 1, 2, 7.6),
       ('2018-04-20 15:08', 5, 5, 6.4),
       ('2018-05-19 13:01', 4, 4, 5.1),
       ('2018-05-20 15:08', 5, 1, 9.4),
       ('2018-05-30 13:02', 3, 2, 9.0);
-- Repartition and insert more data to create chunks with the new
-- partitioning
SELECT set_number_partitions('hyper', 3, 'device');
 set_number_partitions 
-----------------------
 
(1 row)

INSERT INTO hyper VALUES
       ('2018-09-19 13:01', 1, 3, 6.1),
       ('2018-09-20 15:08', 4, 5, 10.4),
       ('2018-09-30 13:02', 3, 4, 9.9);
SELECT (_timescaledb_internal.show_chunk(show_chunks)).*
FROM show_chunks('hyper');
 chunk_id | hypertable_id |      schema_name      |      table_name       | relkind |                                            slices                                            
----------+---------------+-----------------------+-----------------------+---------+----------------------------------------------------------------------------------------------
        1 |             1 | _timescaledb_internal | _hyper_1_1_dist_chunk | f       | {"time": [1516320000000000, 1524096000000000], "device": [-9223372036854775808, 1073741823]}
        2 |             1 | _timescaledb_internal | _hyper_1_2_dist_chunk | f       | {"time": [1516320000000000, 1524096000000000], "device": [1073741823, 9223372036854775807]}
        3 |             1 | _timescaledb_internal | _hyper_1_3_dist_chunk | f       | {"time": [1524096000000000, 1531872000000000], "device": [-9223372036854775808, 1073741823]}
        4 |             1 | _timescaledb_internal | _hyper_1_4_dist_chunk | f       | {"time": [1524096000000000, 1531872000000000], "device": [1073741823, 9223372036854775807]}
        5 |             1 | _timescaledb_internal | _hyper_1_5_dist_chunk | f       | {"time": [1531872000000000, 1539648000000000], "device": [-9223372036854775808, 715827882]}
        6 |             1 | _timescaledb_internal | _hyper_1_6_dist_chunk | f       | {"time": [1531872000000000, 1539648000000000], "device": [715827882, 1431655764]}
        7 |             1 | _timescaledb_internal | _hyper_1_7_dist_chunk | f       | {"time": [1531872000000000, 1539648000000000], "device": [1431655764, 9223372036854775807]}
(7 rows)

-- Show how slices are assigned to data nodes. In particular, verify that
-- there are overlapping slices in the closed "space" dimension
SELECT * FROM test.remote_exec('{ data_node_1, data_node_2 }', $$
       SELECT * FROM _timescaledb_catalog.dimension_slice WHERE dimension_id = 2;
$$);
NOTICE:  [data_node_1]: 
       SELECT * FROM _timescaledb_catalog.dimension_slice WHERE dimension_id = 2
NOTICE:  [data_node_1]:
id|dimension_id|         range_start|          range_end
--+------------+--------------------+-------------------
 5|           2|-9223372036854775808|          715827882
 2|           2|-9223372036854775808|         1073741823
 6|           2|          1431655764|9223372036854775807
(3 rows)


NOTICE:  [data_node_2]: 
       SELECT * FROM _timescaledb_catalog.dimension_slice WHERE dimension_id = 2
NOTICE:  [data_node_2]:
id|dimension_id|range_start|          range_end
--+------------+-----------+-------------------
 5|           2|  715827882|         1431655764
 2|           2| 1073741823|9223372036854775807
(2 rows)


 remote_exec 
-------------
 
(1 row)

SELECT * FROM pg2dim_h1_t1;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Fri Jan 19 13:01:00 2018 PST |      1 |        2 |  2.3
 Sat Jan 20 15:05:00 2018 PST |      1 |        3 |  5.3
 Wed Feb 28 15:05:00 2018 PST |      1 |        1 |  5.6
 Mon Feb 19 13:02:00 2018 PST |      2 |        3 |  6.7
 Sat Mar 10 17:02:00 2018 PST |      1 |        6 |  9.1
 Sat Mar 17 12:02:00 2018 PDT |      2 |        2 |  6.7
(6 rows)

SELECT * FROM pg2dim_h1_t2;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Thu Apr 19 13:01:00 2018 PDT |      1 |        2 |  7.6
(1 row)

SELECT * FROM pg2dim_h1_t3;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Wed Sep 19 13:01:00 2018 PDT |      1 |        3 |  6.1
(1 row)

SELECT * FROM pg2dim_h2_t1;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Wed Feb 21 13:01:00 2018 PST |      3 |        4 |  1.5
 Mon Feb 19 13:02:00 2018 PST |      3 |        5 |  3.1
 Thu Mar 08 11:05:00 2018 PST |      6 |        2 |  8.1
 Thu Mar 08 11:05:00 2018 PST |      7 |        4 |  4.6
 Sat Mar 10 17:02:00 2018 PST |      5 |        5 |  5.1
(5 rows)

SELECT * FROM pg2dim_h2_t2;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Fri Apr 20 15:08:00 2018 PDT |      5 |        5 |  6.4
 Sat May 19 13:01:00 2018 PDT |      4 |        4 |  5.1
 Sun May 20 15:08:00 2018 PDT |      5 |        1 |  9.4
 Wed May 30 13:02:00 2018 PDT |      3 |        2 |    9
(4 rows)

SELECT * FROM pg2dim_h2_t3;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Thu Sep 20 15:08:00 2018 PDT |      4 |        5 | 10.4
 Sun Sep 30 13:02:00 2018 PDT |      3 |        4 |  9.9
(2 rows)

SELECT * FROM  _timescaledb_internal._hyper_1_1_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Fri Jan 19 13:01:00 2018 PST |      1 |        2 |  2.3
 Sat Jan 20 15:05:00 2018 PST |      1 |        3 |  5.3
 Wed Feb 28 15:05:00 2018 PST |      1 |        1 |  5.6
 Thu Mar 08 11:05:00 2018 PST |      6 |        2 |  8.1
 Sat Mar 10 17:02:00 2018 PST |      5 |        5 |  5.1
 Sat Mar 10 17:02:00 2018 PST |      1 |        6 |  9.1
(6 rows)

SELECT * FROM  _timescaledb_internal._hyper_1_2_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Wed Feb 21 13:01:00 2018 PST |      3 |        4 |  1.5
 Mon Feb 19 13:02:00 2018 PST |      3 |        5 |  3.1
 Mon Feb 19 13:02:00 2018 PST |      2 |        3 |  6.7
 Thu Mar 08 11:05:00 2018 PST |      7 |        4 |  4.6
 Sat Mar 17 12:02:00 2018 PDT |      2 |        2 |  6.7
(5 rows)

SELECT * FROM  _timescaledb_internal._hyper_1_3_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Thu Apr 19 13:01:00 2018 PDT |      1 |        2 |  7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 |        5 |  6.4
 Sun May 20 15:08:00 2018 PDT |      5 |        1 |  9.4
(3 rows)

SELECT * FROM  _timescaledb_internal._hyper_1_4_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Sat May 19 13:01:00 2018 PDT |      4 |        4 |  5.1
 Wed May 30 13:02:00 2018 PDT |      3 |        2 |    9
(2 rows)

SELECT * FROM  _timescaledb_internal._hyper_1_5_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Wed Sep 19 13:01:00 2018 PDT |      1 |        3 |  6.1
(1 row)

SELECT * FROM  _timescaledb_internal._hyper_1_6_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Thu Sep 20 15:08:00 2018 PDT |      4 |        5 | 10.4
(1 row)

SELECT * FROM  _timescaledb_internal._hyper_1_7_dist_chunk;
             time             | device | location | temp 
------------------------------+--------+----------+------
 Sun Sep 30 13:02:00 2018 PDT |      3 |        4 |  9.9
(1 row)

------------------------------------------------------------------------
-- PARTIAL partitionwise - open "time" dimension covered by GROUP BY.
-- Note that we don't yet support pushing down partials and PG can't
-- do it on partitioned rels.
-----------------------------------------------------------------------
SET enable_partitionwise_aggregate = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1."time", (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1."time"
   ->  HashAggregate
         Output: pg2dim_h1_t1."time", avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1."time"
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                                                                QUERY PLAN                                                                                                 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: hyper."time", (avg(hyper.temp))
   Sort Key: hyper."time"
   ->  HashAggregate
         Output: hyper."time", avg(hyper.temp)
         Group Key: hyper."time"
         ->  Custom Scan (AsyncAppend)
               Output: hyper."time", hyper.temp
               ->  Append
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: hyper_2."time", hyper_2.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk
                           Remote SQL: SELECT "time", temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(19 rows)

-- Show result
SELECT time, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
             time             | avg  
------------------------------+------
 Fri Jan 19 13:01:00 2018 PST |  2.3
 Sat Jan 20 15:05:00 2018 PST |  5.3
 Mon Feb 19 13:02:00 2018 PST |  4.9
 Wed Feb 21 13:01:00 2018 PST |  1.5
 Wed Feb 28 15:05:00 2018 PST |  5.6
 Thu Mar 08 11:05:00 2018 PST | 6.35
 Sat Mar 10 17:02:00 2018 PST |  7.1
 Sat Mar 17 12:02:00 2018 PDT |  6.7
 Thu Apr 19 13:01:00 2018 PDT |  7.6
 Fri Apr 20 15:08:00 2018 PDT |  6.4
 Sat May 19 13:01:00 2018 PDT |  5.1
 Sun May 20 15:08:00 2018 PDT |  9.4
 Wed May 30 13:02:00 2018 PDT |    9
(13 rows)

SET enable_partitionwise_aggregate = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1."time", (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1."time"
   ->  HashAggregate
         Output: pg2dim_h1_t1."time", avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1."time"
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

SELECT time, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
             time             | avg  
------------------------------+------
 Fri Jan 19 13:01:00 2018 PST |  2.3
 Sat Jan 20 15:05:00 2018 PST |  5.3
 Mon Feb 19 13:02:00 2018 PST |  4.9
 Wed Feb 21 13:01:00 2018 PST |  1.5
 Wed Feb 28 15:05:00 2018 PST |  5.6
 Thu Mar 08 11:05:00 2018 PST | 6.35
 Sat Mar 10 17:02:00 2018 PST |  7.1
 Sat Mar 17 12:02:00 2018 PDT |  6.7
 Thu Apr 19 13:01:00 2018 PDT |  7.6
 Fri Apr 20 15:08:00 2018 PDT |  6.4
 Sat May 19 13:01:00 2018 PDT |  5.1
 Sun May 20 15:08:00 2018 PDT |  9.4
 Wed May 30 13:02:00 2018 PDT |    9
(13 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                                                                                                        QUERY PLAN                                                                                                                                        
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize GroupAggregate
   Output: "time", avg(temp)
   Group Key: "time"
   ->  Custom Scan (AsyncAppend)
         Output: "time", (PARTIAL avg(temp))
         ->  Merge Append
               Sort Key: hyper."time"
               ->  Custom Scan (DataNodeScan)
                     Output: hyper."time", (PARTIAL avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                     Remote SQL: SELECT "time", _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY "time" ASC NULLS LAST
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1."time", (PARTIAL avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk
                     Remote SQL: SELECT "time", _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY "time" ASC NULLS LAST
(19 rows)

-- Show result
SELECT time, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
             time             | avg  
------------------------------+------
 Fri Jan 19 13:01:00 2018 PST |  2.3
 Sat Jan 20 15:05:00 2018 PST |  5.3
 Mon Feb 19 13:02:00 2018 PST |  4.9
 Wed Feb 21 13:01:00 2018 PST |  1.5
 Wed Feb 28 15:05:00 2018 PST |  5.6
 Thu Mar 08 11:05:00 2018 PST | 6.35
 Sat Mar 10 17:02:00 2018 PST |  7.1
 Sat Mar 17 12:02:00 2018 PDT |  6.7
 Thu Apr 19 13:01:00 2018 PDT |  7.6
 Fri Apr 20 15:08:00 2018 PDT |  6.4
 Sat May 19 13:01:00 2018 PDT |  5.1
 Sun May 20 15:08:00 2018 PDT |  9.4
 Wed May 30 13:02:00 2018 PDT |    9
(13 rows)

-------------------------------------------------------------------------
-- FULL partitionwise - only closed "space" dimension covered by GROUP
-- BY -- this is always safe to fully push down if chunks do not
-- overlap in the space dimension.
-------------------------------------------------------------------------
SET enable_partitionwise_aggregate = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1.device
   ->  HashAggregate
         Output: pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

SELECT device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
 device |       avg        
--------+------------------
      1 |             5.98
      2 |              6.7
      3 | 4.53333333333333
      4 |              5.1
      5 | 6.96666666666667
      6 |              8.1
      7 |              4.6
(7 rows)

SET timescaledb.enable_per_data_node_queries = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                                                                QUERY PLAN                                                                                                 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: hyper.device, (avg(hyper.temp))
   Sort Key: hyper.device
   ->  HashAggregate
         Output: hyper.device, avg(hyper.temp)
         Group Key: hyper.device
         ->  Custom Scan (AsyncAppend)
               Output: hyper.device, hyper.temp
               ->  Append
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1.device, hyper_1.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: hyper_2.device, hyper_2.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(19 rows)

-- Show result
SELECT device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
 device |       avg        
--------+------------------
      1 |             5.98
      2 |              6.7
      3 | 4.53333333333333
      4 |              5.1
      5 | 6.96666666666667
      6 |              8.1
      7 |              4.6
(7 rows)

SET enable_partitionwise_aggregate = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1.device
   ->  HashAggregate
         Output: pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
                                                                                                                  QUERY PLAN                                                                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: device, (avg(temp))
   ->  Merge Append
         Sort Key: hyper.device
         ->  Custom Scan (DataNodeScan)
               Output: hyper.device, (avg(hyper.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_1
               Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
               Remote SQL: SELECT device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY device ASC NULLS LAST
         ->  Custom Scan (DataNodeScan)
               Output: hyper_1.device, (avg(hyper_1.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_2
               Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
               Remote SQL: SELECT device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY device ASC NULLS LAST
(16 rows)

-- Show result
SELECT device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1
ORDER BY 1;
 device |       avg        
--------+------------------
      1 |             5.98
      2 |              6.7
      3 | 4.53333333333333
      4 |              5.1
      5 | 6.96666666666667
      6 |              8.1
      7 |              4.6
(7 rows)

-- Grouping on something which is not a partitioning dimension should
-- not be pushed down
EXPLAIN (VERBOSE, COSTS OFF)
SELECT location, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1;
                                                                                                                         QUERY PLAN                                                                                                                          
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize HashAggregate
   Output: location, avg(temp)
   Group Key: location
   ->  Custom Scan (AsyncAppend)
         Output: location, (PARTIAL avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: hyper.location, (PARTIAL avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                     Remote SQL: SELECT location, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1.location, (PARTIAL avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                     Remote SQL: SELECT location, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1
(18 rows)

-- Expand query across repartition boundary. This makes it unsafe to
-- push down the FULL agg, so should expect a PARTIAL agg on
-- hypertables
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM pg2dim
GROUP BY 1
ORDER BY 1;
                                   QUERY PLAN                                    
---------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1.device
   ->  Finalize HashAggregate
         Output: pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1.device
         ->  Append
               ->  Partial HashAggregate
                     Output: pg2dim_h1_t1.device, PARTIAL avg(pg2dim_h1_t1.temp)
                     Group Key: pg2dim_h1_t1.device
                     ->  Seq Scan on public.pg2dim_h1_t1
                           Output: pg2dim_h1_t1.device, pg2dim_h1_t1.temp
               ->  Partial HashAggregate
                     Output: pg2dim_h1_t2.device, PARTIAL avg(pg2dim_h1_t2.temp)
                     Group Key: pg2dim_h1_t2.device
                     ->  Seq Scan on public.pg2dim_h1_t2
                           Output: pg2dim_h1_t2.device, pg2dim_h1_t2.temp
               ->  Partial HashAggregate
                     Output: pg2dim_h1_t3.device, PARTIAL avg(pg2dim_h1_t3.temp)
                     Group Key: pg2dim_h1_t3.device
                     ->  Seq Scan on public.pg2dim_h1_t3
                           Output: pg2dim_h1_t3.device, pg2dim_h1_t3.temp
               ->  Partial HashAggregate
                     Output: pg2dim_h2_t1.device, PARTIAL avg(pg2dim_h2_t1.temp)
                     Group Key: pg2dim_h2_t1.device
                     ->  Seq Scan on public.pg2dim_h2_t1
                           Output: pg2dim_h2_t1.device, pg2dim_h2_t1.temp
               ->  Partial HashAggregate
                     Output: pg2dim_h2_t2.device, PARTIAL avg(pg2dim_h2_t2.temp)
                     Group Key: pg2dim_h2_t2.device
                     ->  Seq Scan on public.pg2dim_h2_t2
                           Output: pg2dim_h2_t2.device, pg2dim_h2_t2.temp
               ->  Partial HashAggregate
                     Output: pg2dim_h2_t3.device, PARTIAL avg(pg2dim_h2_t3.temp)
                     Group Key: pg2dim_h2_t3.device
                     ->  Seq Scan on public.pg2dim_h2_t3
                           Output: pg2dim_h2_t3.device, pg2dim_h2_t3.temp
(37 rows)

SELECT device, avg(temp)
FROM pg2dim
WHERE time > '2018-04-19 00:01'
GROUP BY 1
ORDER BY 1;
 device | avg  
--------+------
      1 | 6.85
      3 | 9.45
      4 | 7.75
      5 |  7.9
(4 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper
GROUP BY 1
ORDER BY 1;
                                                                                            QUERY PLAN                                                                                             
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: device, (avg(temp))
   Sort Key: device
   ->  Finalize HashAggregate
         Output: device, avg(temp)
         Group Key: device
         ->  Custom Scan (AsyncAppend)
               Output: device, (PARTIAL avg(temp))
               ->  Append
                     ->  Custom Scan (DataNodeScan)
                           Output: hyper.device, (PARTIAL avg(hyper.temp))
                           Relations: Aggregate on (public.hyper)
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                           Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) GROUP BY 1
                     ->  Custom Scan (DataNodeScan)
                           Output: hyper_1.device, (PARTIAL avg(hyper_1.temp))
                           Relations: Aggregate on (public.hyper)
                           Data node: data_node_2
                           Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                           Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) GROUP BY 1
(21 rows)

-- Show result
SELECT device, avg(temp)
FROM hyper
GROUP BY 1
ORDER BY 1;
 device |       avg        
--------+------------------
      1 |                6
      2 |              6.7
      3 |            5.875
      4 |             7.75
      5 | 6.96666666666667
      6 |              8.1
      7 |              4.6
(7 rows)

-- Restriction on "time", but not including in target list. Again,
-- this time interval covers a repartitioning, so hypertables should
-- not push down FULL aggs
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM pg2dim
WHERE time > '2018-04-19 00:01'
GROUP BY 1
ORDER BY 1;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t2.device, (avg(pg2dim_h1_t2.temp))
   Sort Key: pg2dim_h1_t2.device
   ->  HashAggregate
         Output: pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp)
         Group Key: pg2dim_h1_t2.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" > 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t3
                     Output: pg2dim_h1_t3.device, pg2dim_h1_t3.temp
                     Filter: (pg2dim_h1_t3."time" > 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" > 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t3
                     Output: pg2dim_h2_t3.device, pg2dim_h2_t3.temp
                     Filter: (pg2dim_h2_t3."time" > 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone)
(19 rows)

-- Show result
SELECT device, avg(temp)
FROM pg2dim
GROUP BY 1
ORDER BY 1;
 device |       avg        
--------+------------------
      1 |                6
      2 |              6.7
      3 |            5.875
      4 |             7.75
      5 | 6.96666666666667
      6 |              8.1
      7 |              4.6
(7 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper
WHERE time > '2018-04-19 00:01'
GROUP BY 1
ORDER BY 1;
                                                                                                                                         QUERY PLAN                                                                                                                                          
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize GroupAggregate
   Output: device, avg(temp)
   Group Key: device
   ->  Custom Scan (AsyncAppend)
         Output: device, (PARTIAL avg(temp))
         ->  Merge Append
               Sort Key: hyper.device
               ->  Custom Scan (DataNodeScan)
                     Output: hyper.device, (PARTIAL avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                     Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 3, 4]) AND (("time" > '2018-04-19 00:01:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY device ASC NULLS LAST
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1.device, (PARTIAL avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                     Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 3]) AND (("time" > '2018-04-19 00:01:00-07'::timestamp with time zone)) GROUP BY 1 ORDER BY device ASC NULLS LAST
(19 rows)

SELECT device, avg(temp)
FROM hyper
WHERE time > '2018-04-19 00:01'
GROUP BY 1
ORDER BY 1;
 device | avg  
--------+------
      1 | 6.85
      3 | 9.45
      4 | 7.75
      5 |  7.9
(4 rows)

--------------------------------------------------------------
-- FULL partitionwise - All partition keys covered by GROUP BY
-- This case is always safe to push down
--------------------------------------------------------------
SET enable_partitionwise_aggregate = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
   ->  HashAggregate
         Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

SELECT time, device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg 
------------------------------+--------+-----
 Fri Jan 19 13:01:00 2018 PST |      1 | 2.3
 Sat Jan 20 15:05:00 2018 PST |      1 | 5.3
 Mon Feb 19 13:02:00 2018 PST |      2 | 6.7
 Mon Feb 19 13:02:00 2018 PST |      3 | 3.1
 Wed Feb 21 13:01:00 2018 PST |      3 | 1.5
 Wed Feb 28 15:05:00 2018 PST |      1 | 5.6
 Thu Mar 08 11:05:00 2018 PST |      6 | 8.1
 Thu Mar 08 11:05:00 2018 PST |      7 | 4.6
 Sat Mar 10 17:02:00 2018 PST |      1 | 9.1
 Sat Mar 10 17:02:00 2018 PST |      5 | 5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 | 6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 | 7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 | 6.4
 Sat May 19 13:01:00 2018 PDT |      4 | 5.1
 Sun May 20 15:08:00 2018 PDT |      5 | 9.4
 Wed May 30 13:02:00 2018 PDT |      3 |   9
(16 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                    QUERY PLAN                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: hyper."time", hyper.device, (avg(hyper.temp))
   Sort Key: hyper."time", hyper.device
   ->  HashAggregate
         Output: hyper."time", hyper.device, avg(hyper.temp)
         Group Key: hyper."time", hyper.device
         ->  Custom Scan (AsyncAppend)
               Output: hyper."time", hyper.device, hyper.temp
               ->  Append
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.device, hyper_1.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: hyper_2."time", hyper_2.device, hyper_2.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(19 rows)

SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg 
------------------------------+--------+-----
 Fri Jan 19 13:01:00 2018 PST |      1 | 2.3
 Sat Jan 20 15:05:00 2018 PST |      1 | 5.3
 Mon Feb 19 13:02:00 2018 PST |      2 | 6.7
 Mon Feb 19 13:02:00 2018 PST |      3 | 3.1
 Wed Feb 21 13:01:00 2018 PST |      3 | 1.5
 Wed Feb 28 15:05:00 2018 PST |      1 | 5.6
 Thu Mar 08 11:05:00 2018 PST |      6 | 8.1
 Thu Mar 08 11:05:00 2018 PST |      7 | 4.6
 Sat Mar 10 17:02:00 2018 PST |      1 | 9.1
 Sat Mar 10 17:02:00 2018 PST |      5 | 5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 | 6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 | 7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 | 6.4
 Sat May 19 13:01:00 2018 PDT |      4 | 5.1
 Sun May 20 15:08:00 2018 PDT |      5 | 9.4
 Wed May 30 13:02:00 2018 PDT |      3 |   9
(16 rows)

SET enable_partitionwise_aggregate = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
   ->  HashAggregate
         Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

-- On hypertable, first show partitionwise aggs without per-data node queries
SET timescaledb.enable_per_data_node_queries = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                   QUERY PLAN                                                                                                                    
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Merge Append
   Sort Key: _hyper_1_1_dist_chunk."time", _hyper_1_1_dist_chunk.device
   ->  Foreign Scan
         Output: _hyper_1_1_dist_chunk."time", _hyper_1_1_dist_chunk.device, (avg(_hyper_1_1_dist_chunk.temp))
         Relations: Aggregate on (_timescaledb_internal._hyper_1_1_dist_chunk hyper)
         Data node: data_node_1
         Remote SQL: SELECT "time", device, avg(temp) FROM _timescaledb_internal._hyper_1_1_dist_chunk WHERE (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
   ->  Foreign Scan
         Output: _hyper_1_2_dist_chunk."time", _hyper_1_2_dist_chunk.device, (avg(_hyper_1_2_dist_chunk.temp))
         Relations: Aggregate on (_timescaledb_internal._hyper_1_2_dist_chunk hyper)
         Data node: data_node_2
         Remote SQL: SELECT "time", device, avg(temp) FROM _timescaledb_internal._hyper_1_2_dist_chunk WHERE (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
   ->  Foreign Scan
         Output: _hyper_1_3_dist_chunk."time", _hyper_1_3_dist_chunk.device, (avg(_hyper_1_3_dist_chunk.temp))
         Relations: Aggregate on (_timescaledb_internal._hyper_1_3_dist_chunk hyper)
         Data node: data_node_1
         Remote SQL: SELECT "time", device, avg(temp) FROM _timescaledb_internal._hyper_1_3_dist_chunk WHERE (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
   ->  Foreign Scan
         Output: _hyper_1_4_dist_chunk."time", _hyper_1_4_dist_chunk.device, (avg(_hyper_1_4_dist_chunk.temp))
         Relations: Aggregate on (_timescaledb_internal._hyper_1_4_dist_chunk hyper)
         Data node: data_node_2
         Remote SQL: SELECT "time", device, avg(temp) FROM _timescaledb_internal._hyper_1_4_dist_chunk WHERE (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(22 rows)

-- Show result
SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg 
------------------------------+--------+-----
 Fri Jan 19 13:01:00 2018 PST |      1 | 2.3
 Sat Jan 20 15:05:00 2018 PST |      1 | 5.3
 Mon Feb 19 13:02:00 2018 PST |      2 | 6.7
 Mon Feb 19 13:02:00 2018 PST |      3 | 3.1
 Wed Feb 21 13:01:00 2018 PST |      3 | 1.5
 Wed Feb 28 15:05:00 2018 PST |      1 | 5.6
 Thu Mar 08 11:05:00 2018 PST |      6 | 8.1
 Thu Mar 08 11:05:00 2018 PST |      7 | 4.6
 Sat Mar 10 17:02:00 2018 PST |      1 | 9.1
 Sat Mar 10 17:02:00 2018 PST |      5 | 5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 | 6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 | 7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 | 6.4
 Sat May 19 13:01:00 2018 PDT |      4 | 5.1
 Sun May 20 15:08:00 2018 PDT |      5 | 9.4
 Wed May 30 13:02:00 2018 PDT |      3 |   9
(16 rows)

-- Enable per-data node queries. Aggregate should be pushed down per
-- data node instead of per chunk.
SET timescaledb.enable_per_data_node_queries = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                                   QUERY PLAN                                                                                                                                   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: "time", device, (avg(temp))
   ->  Merge Append
         Sort Key: hyper."time", hyper.device
         ->  Custom Scan (DataNodeScan)
               Output: hyper."time", hyper.device, (avg(hyper.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_1
               Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
               Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
         ->  Custom Scan (DataNodeScan)
               Output: hyper_1."time", hyper_1.device, (avg(hyper_1.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_2
               Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk
               Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(16 rows)

-- Show result
SELECT time, device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg 
------------------------------+--------+-----
 Fri Jan 19 13:01:00 2018 PST |      1 | 2.3
 Sat Jan 20 15:05:00 2018 PST |      1 | 5.3
 Mon Feb 19 13:02:00 2018 PST |      2 | 6.7
 Mon Feb 19 13:02:00 2018 PST |      3 | 3.1
 Wed Feb 21 13:01:00 2018 PST |      3 | 1.5
 Wed Feb 28 15:05:00 2018 PST |      1 | 5.6
 Thu Mar 08 11:05:00 2018 PST |      6 | 8.1
 Thu Mar 08 11:05:00 2018 PST |      7 | 4.6
 Sat Mar 10 17:02:00 2018 PST |      1 | 9.1
 Sat Mar 10 17:02:00 2018 PST |      5 | 5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 | 6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 | 7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 | 6.4
 Sat May 19 13:01:00 2018 PDT |      4 | 5.1
 Sun May 20 15:08:00 2018 PDT |      5 | 9.4
 Wed May 30 13:02:00 2018 PDT |      3 |   9
(16 rows)

-- Still FULL partitionwise when crossing partitioning boundary
-- because we now cover all partitioning keys.
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM pg2dim
GROUP BY 1, 2
ORDER BY 1, 2;
                                       QUERY PLAN                                        
-----------------------------------------------------------------------------------------
 Sort
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
         ->  HashAggregate
               Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp)
               Group Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
         ->  HashAggregate
               Output: pg2dim_h1_t3."time", pg2dim_h1_t3.device, avg(pg2dim_h1_t3.temp)
               Group Key: pg2dim_h1_t3."time", pg2dim_h1_t3.device
               ->  Seq Scan on public.pg2dim_h1_t3
                     Output: pg2dim_h1_t3."time", pg2dim_h1_t3.device, pg2dim_h1_t3.temp
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp)
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
         ->  HashAggregate
               Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, avg(pg2dim_h2_t2.temp)
               Group Key: pg2dim_h2_t2."time", pg2dim_h2_t2.device
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
         ->  HashAggregate
               Output: pg2dim_h2_t3."time", pg2dim_h2_t3.device, avg(pg2dim_h2_t3.temp)
               Group Key: pg2dim_h2_t3."time", pg2dim_h2_t3.device
               ->  Seq Scan on public.pg2dim_h2_t3
                     Output: pg2dim_h2_t3."time", pg2dim_h2_t3.device, pg2dim_h2_t3.temp
(34 rows)

-- Show result
SELECT time, device, avg(temp)
FROM pg2dim
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg  
------------------------------+--------+------
 Fri Jan 19 13:01:00 2018 PST |      1 |  2.3
 Sat Jan 20 15:05:00 2018 PST |      1 |  5.3
 Mon Feb 19 13:02:00 2018 PST |      2 |  6.7
 Mon Feb 19 13:02:00 2018 PST |      3 |  3.1
 Wed Feb 21 13:01:00 2018 PST |      3 |  1.5
 Wed Feb 28 15:05:00 2018 PST |      1 |  5.6
 Thu Mar 08 11:05:00 2018 PST |      6 |  8.1
 Thu Mar 08 11:05:00 2018 PST |      7 |  4.6
 Sat Mar 10 17:02:00 2018 PST |      1 |  9.1
 Sat Mar 10 17:02:00 2018 PST |      5 |  5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 |  6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 |  7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 |  6.4
 Sat May 19 13:01:00 2018 PDT |      4 |  5.1
 Sun May 20 15:08:00 2018 PDT |      5 |  9.4
 Wed May 30 13:02:00 2018 PDT |      3 |    9
 Wed Sep 19 13:01:00 2018 PDT |      1 |  6.1
 Thu Sep 20 15:08:00 2018 PDT |      4 | 10.4
 Sun Sep 30 13:02:00 2018 PDT |      3 |  9.9
(19 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                    QUERY PLAN                                                                                                    
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: "time", device, (avg(temp))
   ->  Merge Append
         Sort Key: hyper."time", hyper.device
         ->  Custom Scan (DataNodeScan)
               Output: hyper."time", hyper.device, (avg(hyper.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_1
               Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
               Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
         ->  Custom Scan (DataNodeScan)
               Output: hyper_1."time", hyper_1.device, (avg(hyper_1.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_2
               Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
               Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) GROUP BY 1, 2 ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(16 rows)

-- Show result
SELECT time, device, avg(temp)
FROM hyper
GROUP BY 1, 2
ORDER BY 1, 2;
             time             | device | avg  
------------------------------+--------+------
 Fri Jan 19 13:01:00 2018 PST |      1 |  2.3
 Sat Jan 20 15:05:00 2018 PST |      1 |  5.3
 Mon Feb 19 13:02:00 2018 PST |      2 |  6.7
 Mon Feb 19 13:02:00 2018 PST |      3 |  3.1
 Wed Feb 21 13:01:00 2018 PST |      3 |  1.5
 Wed Feb 28 15:05:00 2018 PST |      1 |  5.6
 Thu Mar 08 11:05:00 2018 PST |      6 |  8.1
 Thu Mar 08 11:05:00 2018 PST |      7 |  4.6
 Sat Mar 10 17:02:00 2018 PST |      1 |  9.1
 Sat Mar 10 17:02:00 2018 PST |      5 |  5.1
 Sat Mar 17 12:02:00 2018 PDT |      2 |  6.7
 Thu Apr 19 13:01:00 2018 PDT |      1 |  7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 |  6.4
 Sat May 19 13:01:00 2018 PDT |      4 |  5.1
 Sun May 20 15:08:00 2018 PDT |      5 |  9.4
 Wed May 30 13:02:00 2018 PDT |      3 |    9
 Wed Sep 19 13:01:00 2018 PDT |      1 |  6.1
 Thu Sep 20 15:08:00 2018 PDT |      4 | 10.4
 Sun Sep 30 13:02:00 2018 PDT |      3 |  9.9
(19 rows)

-- Only one chunk per data node, still uses per-data node plan.  Not
-- choosing pushed down aggregate plan here, probably due to costing.
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE time BETWEEN '2018-04-19 00:01' AND '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                                                              QUERY PLAN                                                                                                                                                              
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: "time", device, (avg(temp))
   ->  Merge Append
         Sort Key: hyper."time", hyper.device
         ->  GroupAggregate
               Output: hyper."time", hyper.device, avg(hyper.temp)
               Group Key: hyper."time", hyper.device
               ->  Custom Scan (DataNodeScan) on public.hyper
                     Output: hyper."time", hyper.device, hyper.temp
                     Data node: data_node_1
                     Chunks: _hyper_1_3_dist_chunk
                     Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2]) AND (("time" >= '2018-04-19 00:01:00-07'::timestamp with time zone)) AND (("time" <= '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
         ->  GroupAggregate
               Output: hyper_1."time", hyper_1.device, avg(hyper_1.temp)
               Group Key: hyper_1."time", hyper_1.device
               ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                     Output: hyper_1."time", hyper_1.device, hyper_1.temp
                     Data node: data_node_2
                     Chunks: _hyper_1_4_dist_chunk
                     Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2]) AND (("time" >= '2018-04-19 00:01:00-07'::timestamp with time zone)) AND (("time" <= '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(20 rows)

-- Test HAVING qual
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp) AS temp
FROM pg2dim
WHERE time BETWEEN '2018-04-19 00:01' AND '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) > 4
ORDER BY 1, 2;
                                                                                              QUERY PLAN                                                                                               
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 GroupAggregate
   Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp)
   Group Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
   Filter: (avg(pg2dim_h1_t2.temp) > '4'::double precision)
   ->  Sort
         Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
         Sort Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: ((pg2dim_h1_t2."time" >= 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone) AND (pg2dim_h1_t2."time" <= 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone))
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: ((pg2dim_h2_t2."time" >= 'Thu Apr 19 00:01:00 2018 PDT'::timestamp with time zone) AND (pg2dim_h2_t2."time" <= 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone))
(14 rows)

SELECT time, device, avg(temp) AS temp
FROM pg2dim
WHERE time BETWEEN '2018-04-19 00:01' AND '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) > 4
ORDER BY 1, 2;
             time             | device | temp 
------------------------------+--------+------
 Thu Apr 19 13:01:00 2018 PDT |      1 |  7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 |  6.4
 Sat May 19 13:01:00 2018 PDT |      4 |  5.1
 Sun May 20 15:08:00 2018 PDT |      5 |  9.4
 Wed May 30 13:02:00 2018 PDT |      3 |    9
(5 rows)

-- Test HAVING qual. Not choosing pushed down aggregate plan here,
-- probably due to costing.
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp) AS temp
FROM hyper
WHERE time BETWEEN '2018-04-19 00:01' AND '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) > 4
ORDER BY 1, 2;
                                                                                                                                                              QUERY PLAN                                                                                                                                                              
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: "time", device, (avg(temp))
   ->  Merge Append
         Sort Key: hyper."time", hyper.device
         ->  GroupAggregate
               Output: hyper."time", hyper.device, avg(hyper.temp)
               Group Key: hyper."time", hyper.device
               Filter: (avg(hyper.temp) > '4'::double precision)
               ->  Custom Scan (DataNodeScan) on public.hyper
                     Output: hyper."time", hyper.device, hyper.temp
                     Data node: data_node_1
                     Chunks: _hyper_1_3_dist_chunk
                     Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2]) AND (("time" >= '2018-04-19 00:01:00-07'::timestamp with time zone)) AND (("time" <= '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
         ->  GroupAggregate
               Output: hyper_1."time", hyper_1.device, avg(hyper_1.temp)
               Group Key: hyper_1."time", hyper_1.device
               Filter: (avg(hyper_1.temp) > '4'::double precision)
               ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                     Output: hyper_1."time", hyper_1.device, hyper_1.temp
                     Data node: data_node_2
                     Chunks: _hyper_1_4_dist_chunk
                     Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2]) AND (("time" >= '2018-04-19 00:01:00-07'::timestamp with time zone)) AND (("time" <= '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(22 rows)

SELECT time, device, avg(temp) AS temp
FROM hyper
WHERE time BETWEEN '2018-04-19 00:01' AND '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) > 4
ORDER BY 1, 2;
             time             | device | temp 
------------------------------+--------+------
 Thu Apr 19 13:01:00 2018 PDT |      1 |  7.6
 Fri Apr 20 15:08:00 2018 PDT |      5 |  6.4
 Sat May 19 13:01:00 2018 PDT |      4 |  5.1
 Sun May 20 15:08:00 2018 PDT |      5 |  9.4
 Wed May 30 13:02:00 2018 PDT |      3 |    9
(5 rows)

-------------------------------------------------------------------
-- date_trunc is a whitelisted bucketing function, so should be pushed
-- down fully.
-------------------------------------------------------------------
-- First with partitionwise aggs disabled
SET enable_partitionwise_aggregate = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT date_trunc('month', time), device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                     QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
 Sort
   Output: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device
   ->  HashAggregate
         Output: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: date_trunc('month'::text, pg2dim_h1_t1."time"), pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: date_trunc('month'::text, pg2dim_h1_t2."time"), pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: date_trunc('month'::text, pg2dim_h2_t1."time"), pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: date_trunc('month'::text, pg2dim_h2_t2."time"), pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                    QUERY PLAN                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: (date_trunc('month'::text, hyper."time")), hyper.device, (avg(hyper.temp))
   Sort Key: (date_trunc('month'::text, hyper."time")), hyper.device
   ->  HashAggregate
         Output: (date_trunc('month'::text, hyper."time")), hyper.device, avg(hyper.temp)
         Group Key: (date_trunc('month'::text, hyper."time")), hyper.device
         ->  Custom Scan (AsyncAppend)
               Output: (date_trunc('month'::text, hyper."time")), hyper.device, hyper.temp
               ->  Append
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: date_trunc('month'::text, hyper_1."time"), hyper_1.device, hyper_1.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: date_trunc('month'::text, hyper_2."time"), hyper_2.device, hyper_2.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(19 rows)

SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device | avg 
------------------------------+--------+-----
 Mon Jan 01 00:00:00 2018 PST |      1 | 3.8
 Thu Feb 01 00:00:00 2018 PST |      1 | 5.6
 Thu Feb 01 00:00:00 2018 PST |      2 | 6.7
 Thu Feb 01 00:00:00 2018 PST |      3 | 2.3
 Thu Mar 01 00:00:00 2018 PST |      1 | 9.1
 Thu Mar 01 00:00:00 2018 PST |      2 | 6.7
 Thu Mar 01 00:00:00 2018 PST |      5 | 5.1
 Thu Mar 01 00:00:00 2018 PST |      6 | 8.1
 Thu Mar 01 00:00:00 2018 PST |      7 | 4.6
 Sun Apr 01 00:00:00 2018 PDT |      1 | 7.6
 Sun Apr 01 00:00:00 2018 PDT |      5 | 6.4
 Tue May 01 00:00:00 2018 PDT |      3 |   9
 Tue May 01 00:00:00 2018 PDT |      4 | 5.1
 Tue May 01 00:00:00 2018 PDT |      5 | 9.4
(14 rows)

-- Now with partitionwise aggs enabled
SET enable_partitionwise_aggregate = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT date_trunc('month', time), device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                     QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
 Sort
   Output: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   Sort Key: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device
   ->  HashAggregate
         Output: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
         Group Key: (date_trunc('month'::text, pg2dim_h1_t1."time")), pg2dim_h1_t1.device
         ->  Append
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: date_trunc('month'::text, pg2dim_h1_t1."time"), pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: date_trunc('month'::text, pg2dim_h1_t2."time"), pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: date_trunc('month'::text, pg2dim_h2_t1."time"), pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: date_trunc('month'::text, pg2dim_h2_t2."time"), pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(19 rows)

-- Show reference result
SELECT date_trunc('month', time), device, avg(temp)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device | avg 
------------------------------+--------+-----
 Mon Jan 01 00:00:00 2018 PST |      1 | 3.8
 Thu Feb 01 00:00:00 2018 PST |      1 | 5.6
 Thu Feb 01 00:00:00 2018 PST |      2 | 6.7
 Thu Feb 01 00:00:00 2018 PST |      3 | 2.3
 Thu Mar 01 00:00:00 2018 PST |      1 | 9.1
 Thu Mar 01 00:00:00 2018 PST |      2 | 6.7
 Thu Mar 01 00:00:00 2018 PST |      5 | 5.1
 Thu Mar 01 00:00:00 2018 PST |      6 | 8.1
 Thu Mar 01 00:00:00 2018 PST |      7 | 4.6
 Sun Apr 01 00:00:00 2018 PDT |      1 | 7.6
 Sun Apr 01 00:00:00 2018 PDT |      5 | 6.4
 Tue May 01 00:00:00 2018 PDT |      3 |   9
 Tue May 01 00:00:00 2018 PDT |      4 | 5.1
 Tue May 01 00:00:00 2018 PDT |      5 | 9.4
(14 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                                                              QUERY PLAN                                                                                                                                                              
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: (date_trunc('month'::text, "time")), device, (avg(temp))
   ->  Merge Append
         Sort Key: (date_trunc('month'::text, hyper."time")), hyper.device
         ->  Custom Scan (DataNodeScan)
               Output: (date_trunc('month'::text, hyper."time")), hyper.device, (avg(hyper.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_1
               Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
               Remote SQL: SELECT date_trunc('month'::text, "time"), device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY date_trunc('month'::text, "time") ASC NULLS LAST, device ASC NULLS LAST
         ->  Custom Scan (DataNodeScan)
               Output: (date_trunc('month'::text, hyper_1."time")), hyper_1.device, (avg(hyper_1.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_2
               Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
               Remote SQL: SELECT date_trunc('month'::text, "time"), device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY date_trunc('month'::text, "time") ASC NULLS LAST, device ASC NULLS LAST
(16 rows)

-- Show result
SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device | avg 
------------------------------+--------+-----
 Mon Jan 01 00:00:00 2018 PST |      1 | 3.8
 Thu Feb 01 00:00:00 2018 PST |      1 | 5.6
 Thu Feb 01 00:00:00 2018 PST |      2 | 6.7
 Thu Feb 01 00:00:00 2018 PST |      3 | 2.3
 Thu Mar 01 00:00:00 2018 PST |      1 | 9.1
 Thu Mar 01 00:00:00 2018 PST |      2 | 6.7
 Thu Mar 01 00:00:00 2018 PST |      5 | 5.1
 Thu Mar 01 00:00:00 2018 PST |      6 | 8.1
 Thu Mar 01 00:00:00 2018 PST |      7 | 4.6
 Sun Apr 01 00:00:00 2018 PDT |      1 | 7.6
 Sun Apr 01 00:00:00 2018 PDT |      5 | 6.4
 Tue May 01 00:00:00 2018 PDT |      3 |   9
 Tue May 01 00:00:00 2018 PDT |      4 | 5.1
 Tue May 01 00:00:00 2018 PDT |      5 | 9.4
(14 rows)

-- Should do FULL partitionwise also across repartition boundary since
-- we cover all partitioning keys
EXPLAIN (VERBOSE, COSTS OFF)
SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                                                     QUERY PLAN                                                                                                                                                     
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize GroupAggregate
   Output: (date_trunc('month'::text, "time")), device, avg(temp)
   Group Key: (date_trunc('month'::text, "time")), device
   ->  Custom Scan (AsyncAppend)
         Output: (date_trunc('month'::text, "time")), device, (PARTIAL avg(temp))
         ->  Merge Append
               Sort Key: (date_trunc('month'::text, hyper."time")), hyper.device
               ->  Custom Scan (DataNodeScan)
                     Output: (date_trunc('month'::text, hyper."time")), hyper.device, (PARTIAL avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                     Remote SQL: SELECT date_trunc('month'::text, "time"), device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) GROUP BY 1, 2 ORDER BY date_trunc('month'::text, "time") ASC NULLS LAST, device ASC NULLS LAST
               ->  Custom Scan (DataNodeScan)
                     Output: (date_trunc('month'::text, hyper_1."time")), hyper_1.device, (PARTIAL avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                     Remote SQL: SELECT date_trunc('month'::text, "time"), device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) GROUP BY 1, 2 ORDER BY date_trunc('month'::text, "time") ASC NULLS LAST, device ASC NULLS LAST
(19 rows)

SELECT date_trunc('month', time), device, avg(temp)
FROM hyper
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device | avg  
------------------------------+--------+------
 Mon Jan 01 00:00:00 2018 PST |      1 |  3.8
 Thu Feb 01 00:00:00 2018 PST |      1 |  5.6
 Thu Feb 01 00:00:00 2018 PST |      2 |  6.7
 Thu Feb 01 00:00:00 2018 PST |      3 |  2.3
 Thu Mar 01 00:00:00 2018 PST |      1 |  9.1
 Thu Mar 01 00:00:00 2018 PST |      2 |  6.7
 Thu Mar 01 00:00:00 2018 PST |      5 |  5.1
 Thu Mar 01 00:00:00 2018 PST |      6 |  8.1
 Thu Mar 01 00:00:00 2018 PST |      7 |  4.6
 Sun Apr 01 00:00:00 2018 PDT |      1 |  7.6
 Sun Apr 01 00:00:00 2018 PDT |      5 |  6.4
 Tue May 01 00:00:00 2018 PDT |      3 |    9
 Tue May 01 00:00:00 2018 PDT |      4 |  5.1
 Tue May 01 00:00:00 2018 PDT |      5 |  9.4
 Sat Sep 01 00:00:00 2018 PDT |      1 |  6.1
 Sat Sep 01 00:00:00 2018 PDT |      3 |  9.9
 Sat Sep 01 00:00:00 2018 PDT |      4 | 10.4
(17 rows)

-- Reference result
SELECT date_trunc('month', time), device, avg(temp)
FROM pg2dim
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device | avg  
------------------------------+--------+------
 Mon Jan 01 00:00:00 2018 PST |      1 |  3.8
 Thu Feb 01 00:00:00 2018 PST |      1 |  5.6
 Thu Feb 01 00:00:00 2018 PST |      2 |  6.7
 Thu Feb 01 00:00:00 2018 PST |      3 |  2.3
 Thu Mar 01 00:00:00 2018 PST |      1 |  9.1
 Thu Mar 01 00:00:00 2018 PST |      2 |  6.7
 Thu Mar 01 00:00:00 2018 PST |      5 |  5.1
 Thu Mar 01 00:00:00 2018 PST |      6 |  8.1
 Thu Mar 01 00:00:00 2018 PST |      7 |  4.6
 Sun Apr 01 00:00:00 2018 PDT |      1 |  7.6
 Sun Apr 01 00:00:00 2018 PDT |      5 |  6.4
 Tue May 01 00:00:00 2018 PDT |      3 |    9
 Tue May 01 00:00:00 2018 PDT |      4 |  5.1
 Tue May 01 00:00:00 2018 PDT |      5 |  9.4
 Sat Sep 01 00:00:00 2018 PDT |      1 |  6.1
 Sat Sep 01 00:00:00 2018 PDT |      3 |  9.9
 Sat Sep 01 00:00:00 2018 PDT |      4 | 10.4
(17 rows)

-- Show result by year
SELECT date_trunc('year', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
          date_trunc          | device |       avg        
------------------------------+--------+------------------
 Mon Jan 01 00:00:00 2018 PST |      1 |             5.98
 Mon Jan 01 00:00:00 2018 PST |      2 |              6.7
 Mon Jan 01 00:00:00 2018 PST |      3 | 4.53333333333333
 Mon Jan 01 00:00:00 2018 PST |      4 |              5.1
 Mon Jan 01 00:00:00 2018 PST |      5 | 6.96666666666667
 Mon Jan 01 00:00:00 2018 PST |      6 |              8.1
 Mon Jan 01 00:00:00 2018 PST |      7 |              4.6
(7 rows)

-------------------------------------------------------
-- Test time_bucket (only supports up to days grouping)
-------------------------------------------------------
SET enable_partitionwise_aggregate = OFF;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time_bucket('1 day', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                    QUERY PLAN                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Sort
   Output: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device, (avg(hyper.temp))
   Sort Key: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device
   ->  HashAggregate
         Output: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device, avg(hyper.temp)
         Group Key: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device
         ->  Custom Scan (AsyncAppend)
               Output: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device, hyper.temp
               ->  Append
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: time_bucket('@ 1 day'::interval, hyper_1."time"), hyper_1.device, hyper_1.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: time_bucket('@ 1 day'::interval, hyper_2."time"), hyper_2.device, hyper_2.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(19 rows)

-- Show result
SELECT time_bucket('1 day', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
         time_bucket          | device | avg 
------------------------------+--------+-----
 Thu Jan 18 16:00:00 2018 PST |      1 | 2.3
 Fri Jan 19 16:00:00 2018 PST |      1 | 5.3
 Sun Feb 18 16:00:00 2018 PST |      2 | 6.7
 Sun Feb 18 16:00:00 2018 PST |      3 | 3.1
 Tue Feb 20 16:00:00 2018 PST |      3 | 1.5
 Tue Feb 27 16:00:00 2018 PST |      1 | 5.6
 Wed Mar 07 16:00:00 2018 PST |      6 | 8.1
 Wed Mar 07 16:00:00 2018 PST |      7 | 4.6
 Sat Mar 10 16:00:00 2018 PST |      1 | 9.1
 Sat Mar 10 16:00:00 2018 PST |      5 | 5.1
 Fri Mar 16 17:00:00 2018 PDT |      2 | 6.7
 Wed Apr 18 17:00:00 2018 PDT |      1 | 7.6
 Thu Apr 19 17:00:00 2018 PDT |      5 | 6.4
 Fri May 18 17:00:00 2018 PDT |      4 | 5.1
 Sat May 19 17:00:00 2018 PDT |      5 | 9.4
 Tue May 29 17:00:00 2018 PDT |      3 |   9
(16 rows)

SET enable_partitionwise_aggregate = ON;
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time_bucket('1 day', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
                                                                                                                                                                           QUERY PLAN                                                                                                                                                                           
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: (time_bucket('@ 1 day'::interval, "time")), device, (avg(temp))
   ->  Merge Append
         Sort Key: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device
         ->  Custom Scan (DataNodeScan)
               Output: (time_bucket('@ 1 day'::interval, hyper."time")), hyper.device, (avg(hyper.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_1
               Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
               Remote SQL: SELECT public.time_bucket('@ 1 day'::interval, "time"), device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY public.time_bucket('1 day'::interval, "time") ASC NULLS LAST, device ASC NULLS LAST
         ->  Custom Scan (DataNodeScan)
               Output: (time_bucket('@ 1 day'::interval, hyper_1."time")), hyper_1.device, (avg(hyper_1.temp))
               Relations: Aggregate on (public.hyper)
               Data node: data_node_2
               Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
               Remote SQL: SELECT public.time_bucket('@ 1 day'::interval, "time"), device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1, 2 ORDER BY public.time_bucket('1 day'::interval, "time") ASC NULLS LAST, device ASC NULLS LAST
(16 rows)

-- Show result
SELECT time_bucket('1 day', time), device, avg(temp)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
ORDER BY 1, 2;
         time_bucket          | device | avg 
------------------------------+--------+-----
 Thu Jan 18 16:00:00 2018 PST |      1 | 2.3
 Fri Jan 19 16:00:00 2018 PST |      1 | 5.3
 Sun Feb 18 16:00:00 2018 PST |      2 | 6.7
 Sun Feb 18 16:00:00 2018 PST |      3 | 3.1
 Tue Feb 20 16:00:00 2018 PST |      3 | 1.5
 Tue Feb 27 16:00:00 2018 PST |      1 | 5.6
 Wed Mar 07 16:00:00 2018 PST |      6 | 8.1
 Wed Mar 07 16:00:00 2018 PST |      7 | 4.6
 Sat Mar 10 16:00:00 2018 PST |      1 | 9.1
 Sat Mar 10 16:00:00 2018 PST |      5 | 5.1
 Fri Mar 16 17:00:00 2018 PDT |      2 | 6.7
 Wed Apr 18 17:00:00 2018 PDT |      1 | 7.6
 Thu Apr 19 17:00:00 2018 PDT |      5 | 6.4
 Fri May 18 17:00:00 2018 PDT |      4 | 5.1
 Sat May 19 17:00:00 2018 PDT |      5 | 9.4
 Tue May 29 17:00:00 2018 PDT |      3 |   9
(16 rows)

---------------------------------------------------------------------
-- Test expressions that either aren't pushed down or only pushed down
-- in parts
---------------------------------------------------------------------
-- Create a custom aggregate that does not exist on the data nodes
CREATE AGGREGATE custom_sum(int4) (
    SFUNC = int4_sum,
    STYPE = int8
);
-- sum contains random(), so not pushed down to data nodes
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), sum(temp * (random() <= 1)::int) as sum
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                      QUERY PLAN                                                                                       
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp)), (sum((pg2dim_h1_t1.temp * (((random() <= '1'::double precision))::integer)::double precision)))
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp), sum((pg2dim_h1_t1.temp * (((random() <= '1'::double precision))::integer)::double precision))
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
         ->  HashAggregate
               Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp), sum((pg2dim_h1_t2.temp * (((random() <= '1'::double precision))::integer)::double precision))
               Group Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp), sum((pg2dim_h2_t1.temp * (((random() <= '1'::double precision))::integer)::double precision))
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
         ->  HashAggregate
               Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, avg(pg2dim_h2_t2.temp), sum((pg2dim_h2_t2.temp * (((random() <= '1'::double precision))::integer)::double precision))
               Group Key: pg2dim_h2_t2."time", pg2dim_h2_t2.device
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(27 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), sum(temp * (random() <= 1)::int) as sum
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                                                               QUERY PLAN                                                                                                                                
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp)), (sum((temp * (((random() <= '1'::double precision))::integer)::double precision)))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp)), (sum((temp * (((random() <= '1'::double precision))::integer)::double precision)))
         ->  Merge Append
               Sort Key: hyper."time", hyper.device
               ->  GroupAggregate
                     Output: hyper."time", hyper.device, avg(hyper.temp), sum((hyper.temp * (((random() <= '1'::double precision))::integer)::double precision))
                     Group Key: hyper."time", hyper.device
                     ->  Custom Scan (DataNodeScan) on public.hyper
                           Output: hyper."time", hyper.device, hyper.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
               ->  GroupAggregate
                     Output: hyper_1."time", hyper_1.device, avg(hyper_1.temp), sum((hyper_1.temp * (((random() <= '1'::double precision))::integer)::double precision))
                     Group Key: hyper_1."time", hyper_1.device
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.device, hyper_1.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(22 rows)

-- Pushed down with non-pushable expression taken out
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), random() * device as rand_dev, custom_sum(device)
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                  QUERY PLAN                                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp)), ((random() * (pg2dim_h1_t1.device)::double precision)), (custom_sum(pg2dim_h1_t1.device))
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp), (random() * (pg2dim_h1_t1.device)::double precision), custom_sum(pg2dim_h1_t1.device)
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               ->  Append
                     ->  Seq Scan on public.pg2dim_h1_t1
                           Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                           Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
                     ->  Seq Scan on public.pg2dim_h1_t2
                           Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                           Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp), (random() * (pg2dim_h2_t1.device)::double precision), custom_sum(pg2dim_h2_t1.device)
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               ->  Append
                     ->  Seq Scan on public.pg2dim_h2_t1
                           Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                           Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
                     ->  Seq Scan on public.pg2dim_h2_t2
                           Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                           Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(23 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), random() * device as rand_dev, custom_sum(device)
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                                    QUERY PLAN                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp)), ((random() * (device)::double precision)), (custom_sum(device))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp)), ((random() * (device)::double precision)), (custom_sum(device))
         ->  Append
               ->  HashAggregate
                     Output: hyper."time", hyper.device, avg(hyper.temp), (random() * (hyper.device)::double precision), custom_sum(hyper.device)
                     Group Key: hyper."time", hyper.device
                     ->  Custom Scan (DataNodeScan) on public.hyper
                           Output: hyper."time", hyper.device, hyper.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
               ->  HashAggregate
                     Output: hyper_1."time", hyper_1.device, avg(hyper_1.temp), (random() * (hyper_1.device)::double precision), custom_sum(hyper_1.device)
                     Group Key: hyper_1."time", hyper_1.device
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.device, hyper_1.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(21 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), sum(temp) * random() * device as sum_temp
FROM pg2dim
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) * custom_sum(device) > 0.8
LIMIT 1;
                                                                               QUERY PLAN                                                                                
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp)), (((sum(pg2dim_h1_t1.temp) * random()) * (pg2dim_h1_t1.device)::double precision))
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp), ((sum(pg2dim_h1_t1.temp) * random()) * (pg2dim_h1_t1.device)::double precision)
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               Filter: ((avg(pg2dim_h1_t1.temp) * (custom_sum(pg2dim_h1_t1.device))::double precision) > '0.8'::double precision)
               ->  Append
                     ->  Seq Scan on public.pg2dim_h1_t1
                           Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                           Filter: (pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
                     ->  Seq Scan on public.pg2dim_h1_t2
                           Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                           Filter: (pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp), ((sum(pg2dim_h2_t1.temp) * random()) * (pg2dim_h2_t1.device)::double precision)
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               Filter: ((avg(pg2dim_h2_t1.temp) * (custom_sum(pg2dim_h2_t1.device))::double precision) > '0.8'::double precision)
               ->  Append
                     ->  Seq Scan on public.pg2dim_h2_t1
                           Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                           Filter: (pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
                     ->  Seq Scan on public.pg2dim_h2_t2
                           Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                           Filter: (pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone)
(25 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp), sum(temp) * random() * device as sum_temp
FROM hyper
WHERE time < '2018-06-01 00:00'
GROUP BY 1, 2
HAVING avg(temp) * custom_sum(device) > 0.8
LIMIT 1;
                                                                                                    QUERY PLAN                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp)), (((sum(temp) * random()) * (device)::double precision))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp)), (((sum(temp) * random()) * (device)::double precision))
         ->  Append
               ->  HashAggregate
                     Output: hyper."time", hyper.device, avg(hyper.temp), ((sum(hyper.temp) * random()) * (hyper.device)::double precision)
                     Group Key: hyper."time", hyper.device
                     Filter: ((avg(hyper.temp) * (custom_sum(hyper.device))::double precision) > '0.8'::double precision)
                     ->  Custom Scan (DataNodeScan) on public.hyper
                           Output: hyper."time", hyper.device, hyper.temp
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
               ->  HashAggregate
                     Output: hyper_1."time", hyper_1.device, avg(hyper_1.temp), ((sum(hyper_1.temp) * random()) * (hyper_1.device)::double precision)
                     Group Key: hyper_1."time", hyper_1.device
                     Filter: ((avg(hyper_1.temp) * (custom_sum(hyper_1.device))::double precision) > '0.8'::double precision)
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.device, hyper_1.temp
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone))
(23 rows)

-- not pushed down because of non-shippable expression on the
-- underlying rel
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM pg2dim
WHERE (pg2dim.temp * random() <= 20)
AND time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                  QUERY PLAN                                                                                   
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: ((pg2dim_h1_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone) AND ((pg2dim_h1_t1.temp * random()) <= '20'::double precision))
         ->  HashAggregate
               Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp)
               Group Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: ((pg2dim_h1_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone) AND ((pg2dim_h1_t2.temp * random()) <= '20'::double precision))
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp)
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: ((pg2dim_h2_t1."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone) AND ((pg2dim_h2_t1.temp * random()) <= '20'::double precision))
         ->  HashAggregate
               Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, avg(pg2dim_h2_t2.temp)
               Group Key: pg2dim_h2_t2."time", pg2dim_h2_t2.device
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: ((pg2dim_h2_t2."time" < 'Fri Jun 01 00:00:00 2018 PDT'::timestamp with time zone) AND ((pg2dim_h2_t2.temp * random()) <= '20'::double precision))
(27 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE (hyper.temp * random() <= 20)
AND time < '2018-06-01 00:00'
GROUP BY 1, 2
LIMIT 1;
                                                                                                                               QUERY PLAN                                                                                                                                
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: hyper."time", hyper.device, (avg(hyper.temp))
   ->  GroupAggregate
         Output: hyper."time", hyper.device, avg(hyper.temp)
         Group Key: hyper."time", hyper.device
         ->  Custom Scan (AsyncAppend)
               Output: hyper."time", hyper.device, hyper.temp
               ->  Merge Append
                     Sort Key: hyper_1."time", hyper_1.device
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_1
                           Output: hyper_1."time", hyper_1.device, hyper_1.temp
                           Filter: ((hyper_1.temp * random()) <= '20'::double precision)
                           Data node: data_node_1
                           Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
                     ->  Custom Scan (DataNodeScan) on public.hyper hyper_2
                           Output: hyper_2."time", hyper_2.device, hyper_2.temp
                           Filter: ((hyper_2.temp * random()) <= '20'::double precision)
                           Data node: data_node_2
                           Chunks: _hyper_1_4_dist_chunk, _hyper_1_2_dist_chunk
                           Remote SQL: SELECT "time", device, temp FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[2, 1]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) ORDER BY "time" ASC NULLS LAST, device ASC NULLS LAST
(21 rows)

-- contains whitelisted time expressions
SELECT test_override_pushdown_timestamptz('2018-06-01 00:00'::timestamptz);
 test_override_pushdown_timestamptz 
------------------------------------
 
(1 row)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM pg2dim
WHERE time < Now( ) - INTERVAL '3 days'
GROUP BY 1, 2
LIMIT 1;
                                       QUERY PLAN                                        
-----------------------------------------------------------------------------------------
 Limit
   Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, (avg(pg2dim_h1_t1.temp))
   ->  Append
         ->  HashAggregate
               Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, avg(pg2dim_h1_t1.temp)
               Group Key: pg2dim_h1_t1."time", pg2dim_h1_t1.device
               ->  Seq Scan on public.pg2dim_h1_t1
                     Output: pg2dim_h1_t1."time", pg2dim_h1_t1.device, pg2dim_h1_t1.temp
                     Filter: (pg2dim_h1_t1."time" < (now() - '@ 3 days'::interval))
         ->  HashAggregate
               Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, avg(pg2dim_h1_t2.temp)
               Group Key: pg2dim_h1_t2."time", pg2dim_h1_t2.device
               ->  Seq Scan on public.pg2dim_h1_t2
                     Output: pg2dim_h1_t2."time", pg2dim_h1_t2.device, pg2dim_h1_t2.temp
                     Filter: (pg2dim_h1_t2."time" < (now() - '@ 3 days'::interval))
         ->  HashAggregate
               Output: pg2dim_h1_t3."time", pg2dim_h1_t3.device, avg(pg2dim_h1_t3.temp)
               Group Key: pg2dim_h1_t3."time", pg2dim_h1_t3.device
               ->  Seq Scan on public.pg2dim_h1_t3
                     Output: pg2dim_h1_t3."time", pg2dim_h1_t3.device, pg2dim_h1_t3.temp
                     Filter: (pg2dim_h1_t3."time" < (now() - '@ 3 days'::interval))
         ->  HashAggregate
               Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, avg(pg2dim_h2_t1.temp)
               Group Key: pg2dim_h2_t1."time", pg2dim_h2_t1.device
               ->  Seq Scan on public.pg2dim_h2_t1
                     Output: pg2dim_h2_t1."time", pg2dim_h2_t1.device, pg2dim_h2_t1.temp
                     Filter: (pg2dim_h2_t1."time" < (now() - '@ 3 days'::interval))
         ->  HashAggregate
               Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, avg(pg2dim_h2_t2.temp)
               Group Key: pg2dim_h2_t2."time", pg2dim_h2_t2.device
               ->  Seq Scan on public.pg2dim_h2_t2
                     Output: pg2dim_h2_t2."time", pg2dim_h2_t2.device, pg2dim_h2_t2.temp
                     Filter: (pg2dim_h2_t2."time" < (now() - '@ 3 days'::interval))
         ->  HashAggregate
               Output: pg2dim_h2_t3."time", pg2dim_h2_t3.device, avg(pg2dim_h2_t3.temp)
               Group Key: pg2dim_h2_t3."time", pg2dim_h2_t3.device
               ->  Seq Scan on public.pg2dim_h2_t3
                     Output: pg2dim_h2_t3."time", pg2dim_h2_t3.device, pg2dim_h2_t3.temp
                     Filter: (pg2dim_h2_t3."time" < (now() - '@ 3 days'::interval))
(39 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, device, avg(temp)
FROM hyper
WHERE time < Now( ) - INTERVAL '3 days'
GROUP BY 1, 2
LIMIT 1;
                                                                                                                    QUERY PLAN                                                                                                                    
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: hyper."time", hyper.device, (avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) AND (("time" < (('2018-06-01 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1."time", hyper_1.device, (avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) AND (("time" < (('2018-06-01 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
(17 rows)

-- Verify that repeated runs of the same plan will get different timestamps
PREPARE timestamp_pushdown_test AS
SELECT time, device, avg(temp)
FROM hyper
WHERE time < now() - INTERVAL '3 days'
GROUP BY 1, 2
LIMIT 1;
EXPLAIN (VERBOSE, COSTS OFF)
EXECUTE timestamp_pushdown_test;
                                                                                                                    QUERY PLAN                                                                                                                    
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: hyper."time", hyper.device, (avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) AND (("time" < (('2018-06-01 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1."time", hyper_1.device, (avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) AND (("time" < (('2018-06-01 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
(17 rows)

SELECT test_override_pushdown_timestamptz('2019-10-15 00:00'::timestamptz);
 test_override_pushdown_timestamptz 
------------------------------------
 
(1 row)

EXPLAIN (VERBOSE, COSTS OFF)
EXECUTE timestamp_pushdown_test;
                                                                                                                    QUERY PLAN                                                                                                                    
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit
   Output: "time", device, (avg(temp))
   ->  Custom Scan (AsyncAppend)
         Output: "time", device, (avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: hyper."time", hyper.device, (avg(hyper.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_1
                     Chunks: _hyper_1_1_dist_chunk, _hyper_1_3_dist_chunk, _hyper_1_5_dist_chunk, _hyper_1_7_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3, 4]) AND (("time" < (('2019-10-15 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
               ->  Custom Scan (DataNodeScan)
                     Output: hyper_1."time", hyper_1.device, (avg(hyper_1.temp))
                     Relations: Aggregate on (public.hyper)
                     Data node: data_node_2
                     Chunks: _hyper_1_2_dist_chunk, _hyper_1_4_dist_chunk, _hyper_1_6_dist_chunk
                     Remote SQL: SELECT "time", device, avg(temp) FROM public.hyper WHERE _timescaledb_internal.chunks_in(hyper, ARRAY[1, 2, 3]) AND (("time" < (('2019-10-15 00:00:00-07'::timestamptz) - '3 days'::interval))) GROUP BY 1, 2
(17 rows)

-- Test one-dimensional push down
CREATE TABLE hyper1d (time timestamptz, device int, temp float);
SELECT * FROM create_distributed_hypertable('hyper1d', 'time', chunk_time_interval => '3 months'::interval);
NOTICE:  adding not-null constraint to column "time"
 hypertable_id | schema_name | table_name | created 
---------------+-------------+------------+---------
             2 | public      | hyper1d    | t
(1 row)

INSERT INTO hyper1d VALUES
       ('2018-01-19 13:01', 1, 2.3),
       ('2018-01-20 15:05', 1, 5.3),
       ('2018-02-21 13:01', 3, 1.5),
       ('2018-02-28 15:05', 1, 5.6),
       ('2018-02-19 13:02', 3, 3.1),
       ('2018-02-19 13:02', 3, 6.7),
       ('2018-04-19 13:01', 1, 7.6),
       ('2018-04-20 15:08', 3, 8.4),
       ('2018-05-19 13:01', 1, 5.1),
       ('2018-05-20 15:08', 1, 9.4),
       ('2018-05-30 13:02', 3, 9.0),
       ('2018-09-19 13:01', 1, 6.1),
       ('2018-09-20 15:08', 2, 10.4),
       ('2018-09-30 13:02', 3, 9.9);
SET enable_partitionwise_aggregate = ON;
SET timescaledb.enable_per_data_node_queries = ON;
-- Covering partitioning dimension is always safe to push down.
EXPLAIN (VERBOSE, COSTS OFF)
SELECT time, avg(temp)
FROM hyper1d
GROUP BY 1;
                                                                  QUERY PLAN                                                                   
-----------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)
   Output: "time", (avg(temp))
   ->  Append
         ->  Custom Scan (DataNodeScan)
               Output: hyper1d."time", (avg(hyper1d.temp))
               Relations: Aggregate on (public.hyper1d)
               Data node: data_node_1
               Chunks: _hyper_2_8_dist_chunk, _hyper_2_10_dist_chunk
               Remote SQL: SELECT "time", avg(temp) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[5, 6]) GROUP BY 1
         ->  Custom Scan (DataNodeScan)
               Output: hyper1d_1."time", (avg(hyper1d_1.temp))
               Relations: Aggregate on (public.hyper1d)
               Data node: data_node_2
               Chunks: _hyper_2_9_dist_chunk
               Remote SQL: SELECT "time", avg(temp) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[4]) GROUP BY 1
(15 rows)

EXPLAIN (VERBOSE, COSTS OFF)
SELECT time_bucket('1 day', time), avg(temp)
FROM hyper1d
GROUP BY 1;
                                                                                                             QUERY PLAN                                                                                                             
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize HashAggregate
   Output: (time_bucket('@ 1 day'::interval, "time")), avg(temp)
   Group Key: (time_bucket('@ 1 day'::interval, "time"))
   ->  Custom Scan (AsyncAppend)
         Output: (time_bucket('@ 1 day'::interval, "time")), (PARTIAL avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: (time_bucket('@ 1 day'::interval, hyper1d."time")), (PARTIAL avg(hyper1d.temp))
                     Relations: Aggregate on (public.hyper1d)
                     Data node: data_node_1
                     Chunks: _hyper_2_8_dist_chunk, _hyper_2_10_dist_chunk
                     Remote SQL: SELECT public.time_bucket('@ 1 day'::interval, "time"), _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[5, 6]) GROUP BY 1
               ->  Custom Scan (DataNodeScan)
                     Output: (time_bucket('@ 1 day'::interval, hyper1d_1."time")), (PARTIAL avg(hyper1d_1.temp))
                     Relations: Aggregate on (public.hyper1d)
                     Data node: data_node_2
                     Chunks: _hyper_2_9_dist_chunk
                     Remote SQL: SELECT public.time_bucket('@ 1 day'::interval, "time"), _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[4]) GROUP BY 1
(18 rows)

--- Only one chunk in query => safe to fully push down although not on
--- a partitioning dimension.
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper1d
WHERE time < '2018-04-01 00:00'
GROUP BY 1;
                                                                                             QUERY PLAN                                                                                             
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (DataNodeScan)
   Output: hyper1d.device, (avg(hyper1d.temp))
   Relations: Aggregate on (public.hyper1d)
   Data node: data_node_1
   Chunks: _hyper_2_8_dist_chunk
   Remote SQL: SELECT device, avg(temp) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[5]) AND (("time" < '2018-04-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1
(6 rows)

-- Two chunks in query => not safe to fully push down when not
-- grouping on partitioning dimension
EXPLAIN (VERBOSE, COSTS OFF)
SELECT device, avg(temp)
FROM hyper1d
WHERE time < '2018-06-01 00:00'
GROUP BY 1;
                                                                                                                         QUERY PLAN                                                                                                                         
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Finalize HashAggregate
   Output: device, avg(temp)
   Group Key: device
   ->  Custom Scan (AsyncAppend)
         Output: device, (PARTIAL avg(temp))
         ->  Append
               ->  Custom Scan (DataNodeScan)
                     Output: hyper1d.device, (PARTIAL avg(hyper1d.temp))
                     Relations: Aggregate on (public.hyper1d)
                     Data node: data_node_1
                     Chunks: _hyper_2_8_dist_chunk
                     Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[5]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1
               ->  Custom Scan (DataNodeScan)
                     Output: hyper1d_1.device, (PARTIAL avg(hyper1d_1.temp))
                     Relations: Aggregate on (public.hyper1d)
                     Data node: data_node_2
                     Chunks: _hyper_2_9_dist_chunk
                     Remote SQL: SELECT device, _timescaledb_internal.partialize_agg(avg(temp)) FROM public.hyper1d WHERE _timescaledb_internal.chunks_in(hyper1d, ARRAY[4]) AND (("time" < '2018-06-01 00:00:00-07'::timestamp with time zone)) GROUP BY 1
(18 rows)

